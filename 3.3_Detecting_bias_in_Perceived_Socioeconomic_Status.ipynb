{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We upload the data for the protected variable Perceived Socioeconomic Status got during the preprocessing as a dataframe. The names of the columns are: name, text_x, text_y, Low qualification."
      ],
      "metadata": {
        "id": "NMBIPBnDV-_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "profesion_df = pd.read_excel('/content/ex_profesions.xlsx')\n",
        "profesion_df"
      ],
      "metadata": {
        "id": "HL-UEsWdOdLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create two dataframes, one for for profesions that require high qualification, and another one for those that require low qualification. The qualifications may be perceived as a socioeconomic status by medical staff."
      ],
      "metadata": {
        "id": "T9Pgr44aXbb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "high = profesion_df[profesion_df['Low qualification'] == 0]\n",
        "low = profesion_df[profesion_df['Low qualification'] == 1]"
      ],
      "metadata": {
        "id": "u4hBjXKqOsOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descriptive statistics"
      ],
      "metadata": {
        "id": "DvA3tNPIVzu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We calculate the length of the text of perceived high socioeconomic patients' notes."
      ],
      "metadata": {
        "id": "3CTMGjqcZ9_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "high['text_length'] = high['text_y'].str.len()\n",
        "high\n",
        "\n",
        "high['text_length'].hist()\n",
        "plt.title('Histogram of Text Lengths for perceived high socioeconomic patients')\n",
        "plt.xlabel('Length of Text')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "print(\"Average length:\", high['text_length'].mean())\n",
        "print(\"Median length:\", high['text_length'].median())"
      ],
      "metadata": {
        "id": "4MLfpeY6EskT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We count the average and media words of the perceived high socioeconomic patients' notes."
      ],
      "metadata": {
        "id": "nJGdwNjEaHeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "high['word_count'] = high['text_y'].apply(lambda x: len(str(x).split()))\n",
        "high['word_count'].hist()\n",
        "\n",
        "plt.title('Histogram word count for perceived high socioeconomic patients')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "print(\"Average length:\", high['word_count'].mean())\n",
        "print(\"Median length:\", high['word_count'].median())"
      ],
      "metadata": {
        "id": "gMu2wMwCHs4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We calculate the length of the text of perceived low socioeconomic patients' notes."
      ],
      "metadata": {
        "id": "CwYwZvqNaU_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "low['text_length'] = low['text_y'].str.len()\n",
        "\n",
        "low['text_length'].hist()\n",
        "plt.title('Histogram of Text Lengths for perceived low socioeconomic patients')\n",
        "plt.xlabel('Length of Text')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "print(\"Average length:\", low['text_length'].mean())\n",
        "print(\"Median length:\", low['text_length'].median())"
      ],
      "metadata": {
        "id": "G9iY86yHFSLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We count the average and media words of the perceived low socioeconomic patients' notes."
      ],
      "metadata": {
        "id": "pxIWKUggaYof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "low['word_count'] = low['text_y'].apply(lambda x: len(str(x).split()))\n",
        "low['word_count'].hist()\n",
        "\n",
        "plt.title('Histogram word count for perceived low socioeconomic patients')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "print(\"Average length:\", low['word_count'].mean())\n",
        "print(\"Median length:\", low['word_count'].median())"
      ],
      "metadata": {
        "id": "U5KQ3EdvH9I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentiment analysis\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MTfsc0JE-ZHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Pretrained BERT Pysentimiento**"
      ],
      "metadata": {
        "id": "BBgRqfJK-qxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We install and import the necessary libraries."
      ],
      "metadata": {
        "id": "1TM92Fe5acMp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKf7tWxG0ufa"
      },
      "outputs": [],
      "source": [
        "!pip install pysentimiento\n",
        "\n",
        "from pysentimiento import create_analyzer\n",
        "import transformers\n",
        "\n",
        "transformers.logging.set_verbosity(transformers.logging.ERROR)\n",
        "\n",
        "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We apply RoBERTuito for the perceived high and low socioeconomic patients' notes, respectively."
      ],
      "metadata": {
        "id": "wBG_qnCWazPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###Sentiment in texts for perceived high socioeconomic patients:\n",
        "positive_count = 0\n",
        "neutral_count = 0\n",
        "negative_count = 0\n",
        "\n",
        "for index, row in high.iterrows():\n",
        "    text = row['text_y']\n",
        "    result_high = analyzer.predict(text)\n",
        "\n",
        "    if result_high.output == 'POS':\n",
        "        positive_count += 1\n",
        "    elif result_high.output == 'NEU':\n",
        "        neutral_count += 1\n",
        "    elif result_high.output == 'NEG':\n",
        "        negative_count += 1\n",
        "\n",
        "print(\"Positive:\", positive_count)\n",
        "print(\"Neutral:\", neutral_count)\n",
        "print(\"Negative:\", negative_count)"
      ],
      "metadata": {
        "id": "MWph2RDDV4jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Sentiment in texts for perceived low socioeconomic patients:\n",
        "positive_count = 0\n",
        "neutral_count = 0\n",
        "negative_count = 0\n",
        "\n",
        "for index, row in low.iterrows():\n",
        "    text = row['text_y']\n",
        "    result_low = analyzer.predict(text)\n",
        "\n",
        "    if result_low.output == 'POS':\n",
        "        positive_count += 1\n",
        "    elif result_low.output == 'NEU':\n",
        "        neutral_count += 1\n",
        "    elif result_low.output == 'NEG':\n",
        "        negative_count += 1\n",
        "\n",
        "print(\"Positive:\", positive_count)\n",
        "print(\"Neutral:\", neutral_count)\n",
        "print(\"Negative:\", negative_count)"
      ],
      "metadata": {
        "id": "hvnzWuXoXHDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. AFINN with Spanish Lexicon**"
      ],
      "metadata": {
        "id": "eJbc4UMo-yWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We install and import the necessary libraries."
      ],
      "metadata": {
        "id": "hBgXLhCVa5dL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob\n",
        "from textblob import TextBlob, Word\n",
        "\n",
        "!pip install nltk\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import string\n",
        "import os\n",
        "stop_words = set(stopwords.words('spanish'))"
      ],
      "metadata": {
        "id": "YQenbJHzXtqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, we initialise an empty dictionary to store FINN lexicon. Afterwards, we load the AFINN lexicon from a CSV file into a DataFrame. Only the first two columns are used. the, we iterate through each row of the DataFrame, extracting the word and its sentiment score."
      ],
      "metadata": {
        "id": "Gv5_pJMA5l6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "afinn = {}\n",
        "\n",
        "lexicon_df = pd.read_csv('/content/lexico_afinn.csv', header=0)\n",
        "lexicon_df = lexicon_df.iloc[:, :2]\n",
        "\n",
        "for index, row in lexicon_df.iterrows():\n",
        "    word = row['palabra']\n",
        "    sentiment_score_str = row['puntuacion']\n",
        "\n",
        "    try:\n",
        "        sentiment_score = float(sentiment_score_str)\n",
        "        afinn[word] = sentiment_score\n",
        "    except ValueError:\n",
        "        print(f\"Invalid sentiment score '{sentiment_score_str}' for word '{word}'. Skipping row.\")"
      ],
      "metadata": {
        "id": "A5FkDDjPbHYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a function that calculates the sentiment of the text. First, convert the text to lowercase and split it into words. Then, calculate the sentiment score by summing the scores of individual words using the AFINN dictionary."
      ],
      "metadata": {
        "id": "W513wvbr5rS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sentiment(text):\n",
        "    words = text.lower().split()\n",
        "    sentiment_score = sum(afinn.get(word, 0) for word in words)\n",
        "    if sentiment_score > 0:\n",
        "        return 'Positive'\n",
        "    elif sentiment_score < 0:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'"
      ],
      "metadata": {
        "id": "Z17Hwv5Iao04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also upload stopwords for Catalan and add them to those for Spanish."
      ],
      "metadata": {
        "id": "ZZy2WB7w5s9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "catalan_stopwords = catalan_stopwords = [\n",
        "    'de', 'es', 'i', 'a', 'o', 'un', 'una', 'unes', 'uns', 'un', 'tot',\n",
        "    'també', 'altre', 'algun', 'alguna', 'alguns', 'algunes', 'ser', 'és',\n",
        "    'soc', 'ets', 'som', 'estic', 'està', 'estem', 'esteu', 'estan', 'com',\n",
        "    'en', 'per', 'perquè', 'per que', 'estat', 'estava', 'ans', 'abans',\n",
        "    'éssent', 'ambdós', 'però', 'per', 'poder', 'potser', 'puc', 'podem',\n",
        "    'podeu', 'poden', 'vaig', 'va', 'van', 'fer', 'faig', 'fa', 'fem',\n",
        "    'feu', 'fan', 'cada', 'fi', 'inclòs', 'primer', 'des de', 'conseguir',\n",
        "    'consegueixo', 'consigueix', 'consigueixes', 'conseguim', 'consigueixen',\n",
        "    'anar', 'haver', 'tenir', 'tinc', 'te', 'tenim', 'teniu', 'tene', 'el',\n",
        "    'la', 'les', 'els', 'seu', 'aquí', 'meu', 'teu', 'ells', 'elles', 'ens',\n",
        "    'nosaltres', 'vosaltres', 'si', 'dins', 'sols', 'solament', 'saber',\n",
        "    'saps', 'sap', 'sabem', 'sabeu', 'saben', 'últim', 'llarg', 'bastant',\n",
        "    'fas', 'molts', 'aquells', 'aquelles', 'seus', 'llavors', 'sota', 'dalt',\n",
        "    'ús', 'molt', 'era', 'eres', 'erem', 'eren', 'mode', 'bé', 'quant',\n",
        "    'quan', 'on', 'mentre', 'qui', 'amb', 'entre', 'sense', 'jo', 'aquell'\n",
        "]\n",
        "\n",
        "stop_words.update(catalan_stopwords)"
      ],
      "metadata": {
        "id": "wWbbPHM309wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a function to preprocess text data. The text is converted to lower case, punctuation removed, tokenised using TextBlob, lemmatised, and stopwords removed."
      ],
      "metadata": {
        "id": "o5pjuNUz5u-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = TextBlob(text).words\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    lemmatized_tokens = [Word(word).lemmatize() for word in tokens]\n",
        "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "high['preprocessed_text'] = high['text_y'].apply(preprocess_text)\n",
        "low['preprocessed_text'] = low['text_y'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "spTfpwf2bRi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We apply the calculate_sentiment function to the 'preprocessed_text' column of the two dataframes and store the sentiment labels in a new column 'sentiment'."
      ],
      "metadata": {
        "id": "aJR6fERH58cT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "high['sentiment'] = high['preprocessed_text'].apply(lambda text: calculate_sentiment(text))\n",
        "low['sentiment'] = low['preprocessed_text'].apply(lambda text: calculate_sentiment(text))"
      ],
      "metadata": {
        "id": "1A646G9QSUL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We count the number of occurrences of each sentiment label ('Positive', 'Negative', 'Neutral') for women's notes."
      ],
      "metadata": {
        "id": "5gLwtKoa6CYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_counts_high = high['sentiment'].value_counts()\n",
        "sentiment_counts_high"
      ],
      "metadata": {
        "id": "eRc-6JsLSecq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We count the number of occurrences of each sentiment label ('Positive', 'Negative', 'Neutral') for older men's notes."
      ],
      "metadata": {
        "id": "KontzpZd6EMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_counts_low = low['sentiment'].value_counts()\n",
        "sentiment_counts_low"
      ],
      "metadata": {
        "id": "ERbRKim1So41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic modelling and document embedding"
      ],
      "metadata": {
        "id": "XNe1dQ5PV_4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LDA**"
      ],
      "metadata": {
        "id": "3Txq3fyw-4e6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We install and import the necessary libraries."
      ],
      "metadata": {
        "id": "sfgrYFc-6GDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models.ldamodel import LdaModel"
      ],
      "metadata": {
        "id": "2vMTyk_g0VKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use LDA for topic modelling, following the following steps:"
      ],
      "metadata": {
        "id": "-Jv7HRN46IK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Firstly, we tokenize the text\n",
        "def tokenize_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "high['tokenized_text'] = high['preprocessed_text'].apply(tokenize_text)\n",
        "low['tokenized_text'] = low['preprocessed_text'].apply(tokenize_text)\n",
        "\n",
        "#We create dictionary representation of the documents\n",
        "dictionary = Dictionary(high['tokenized_text'])\n",
        "dictionary_un = Dictionary(low['tokenized_text'])\n",
        "\n",
        "dictionary.filter_extremes(no_below=4, no_above=0.7)\n",
        "dictionary_un.filter_extremes(no_below=4, no_above=0.7)\n",
        "\n",
        "#We convert the dictionary to a bag of words corpus\n",
        "corpus_high = [dictionary.doc2bow(doc) for doc in high['tokenized_text']]\n",
        "corpus_low = [dictionary_un.doc2bow(doc) for doc in low['tokenized_text']]"
      ],
      "metadata": {
        "id": "L9EHpEAZTyRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we apply the LDA model, and represent 2 topics for women and men's notes, respectively:"
      ],
      "metadata": {
        "id": "QT7Phaf46VFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_topics = 2\n",
        "\n",
        "# LDA in perceived high socioeconomic patients\n",
        "lda_high = LdaModel(corpus_high, num_topics=num_topics, id2word=dictionary, passes=15)\n",
        "\n",
        "# LDA in perceived low socioeconomic patients\n",
        "lda_low = LdaModel(corpus_low, num_topics=num_topics, id2word=dictionary_un, passes=15)\n",
        "\n",
        "# Topics for the perceived high socioeconomic patients\n",
        "print(\"high Topics:\")\n",
        "for i, topic in lda_high.print_topics(num_words=10):\n",
        "    print(f\"Topic {i}: {topic}\")\n",
        "\n",
        "# Topics for the perceived low socioeconomic patients\n",
        "print(\"\\nlow Topics:\")\n",
        "for i, topic in lda_low.print_topics(num_words=10):\n",
        "    print(f\"Topic {i}: {topic}\")"
      ],
      "metadata": {
        "id": "sBFdrVNKVu1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**t-SNE**"
      ],
      "metadata": {
        "id": "ws8RTL94DdWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We install and import the necessary libraries:"
      ],
      "metadata": {
        "id": "t50ysu976ify"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas scikit-learn\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "6OS9lS9BDg9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We preprocess the text in the age_df dataframe, and then we vectorise it."
      ],
      "metadata": {
        "id": "Ahwna3Wa6hLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "profesion_df['preprocessed_text'] = profesion_df['text_y'].apply(preprocess_text)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(profesion_df['preprocessed_text'])"
      ],
      "metadata": {
        "id": "p6RsZ7kXDy3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We apply T-SNE and plot it."
      ],
      "metadata": {
        "id": "K2Wob0ws6lTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "\n",
        "tsne_results = tsne.fit_transform(tfidf_matrix.toarray())\n",
        "\n",
        "profesion_df['tsne_1'] = tsne_results[:, 0]\n",
        "profesion_df['tsne_2'] = tsne_results[:, 1]\n",
        "\n",
        "#plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = sns.scatterplot(\n",
        "    x='tsne_1', y='tsne_2',\n",
        "    hue='Low qualification',\n",
        "    palette=sns.color_palette(\"tab10\", 2),\n",
        "    data=profesion_df,\n",
        "    legend=\"full\",\n",
        "    alpha=0.6\n",
        ")\n",
        "\n",
        "handles, labels = scatter.get_legend_handles_labels()\n",
        "labels = ['High', 'Low']\n",
        "scatter.legend(handles, labels, title=\"Perceived socioeconomic status\")\n",
        "\n",
        "plt.title('t-SNE visualization of topics for the perceived high vs low socieconomic patients')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oeZrX6HZElky"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}