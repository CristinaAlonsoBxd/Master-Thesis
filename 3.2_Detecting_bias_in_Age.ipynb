{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We upload the data for the protected variable Age got during the preprocessing as a dataframe. The names of the columns are: name, text_y, age, older."
      ],
      "metadata": {
        "id": "hDTbkfNEkgio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "age_df = pd.read_excel('/content/ex_age.xlsx')\n",
        "age_df"
      ],
      "metadata": {
        "id": "HL-UEsWdOdLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create two dataframes, one for older than 60 years old patients ('older') and another one for patients younger than 60 years old ('younger')."
      ],
      "metadata": {
        "id": "_4yo00lOylII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "older = age_df[age_df['older'] == 1]\n",
        "younger = age_df[age_df['older'] == 0]"
      ],
      "metadata": {
        "id": "u4hBjXKqOsOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descriptive statistics"
      ],
      "metadata": {
        "id": "DvA3tNPIVzu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We calculate the length of the text of older patients' notes."
      ],
      "metadata": {
        "id": "6bdJXlee12vK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "older['text_length'] = older['text_y'].str.len()\n",
        "older['text_length'].hist()\n",
        "plt.title('Histogram of Text Lengths for older patients')\n",
        "plt.xlabel('Length of Text')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "print(\"Average length:\", older['text_length'].mean())\n",
        "print(\"Median length:\", older['text_length'].median())"
      ],
      "metadata": {
        "id": "4MLfpeY6EskT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We count the words of the older patients' notes."
      ],
      "metadata": {
        "id": "ACWO-g035mrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "older['word_count'] = older['text_y'].apply(lambda x: len(str(x).split()))\n",
        "older['word_count'].hist()\n",
        "\n",
        "plt.title('Histogram word count for older patients')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "print(\"Average length:\", older['word_count'].mean())\n",
        "print(\"Median length:\", older['word_count'].median())"
      ],
      "metadata": {
        "id": "gMu2wMwCHs4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We calculate the length of the text for younger patients."
      ],
      "metadata": {
        "id": "X9LxzSF55oWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "younger['text_length'] = younger['text_y'].str.len()\n",
        "\n",
        "younger['text_length'].hist()\n",
        "plt.title('Histogram of Text Lengths for younger patients')\n",
        "plt.xlabel('Length of Text')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "print(\"Average length:\", younger['text_length'].mean())\n",
        "print(\"Median length:\", younger['text_length'].median())"
      ],
      "metadata": {
        "id": "G9iY86yHFSLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We count the words of the younger patients' notes."
      ],
      "metadata": {
        "id": "FpIpnr_254yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "younger['word_count'] = younger['text_y'].apply(lambda x: len(str(x).split()))\n",
        "younger['word_count'].hist()\n",
        "\n",
        "plt.title('Histogram word count for younger patients')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "print(\"Average length:\", younger['word_count'].mean())\n",
        "print(\"Median length:\", younger['word_count'].median())"
      ],
      "metadata": {
        "id": "U5KQ3EdvH9I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentiment analysis\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MTfsc0JE-ZHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Pretrained BERT Pysentimiento**"
      ],
      "metadata": {
        "id": "BBgRqfJK-qxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We install and import the necessary libraries."
      ],
      "metadata": {
        "id": "TOY9F0Gz9N4s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKf7tWxG0ufa"
      },
      "outputs": [],
      "source": [
        "!pip install pysentimiento\n",
        "\n",
        "from pysentimiento import create_analyzer\n",
        "import transformers\n",
        "\n",
        "transformers.logging.set_verbosity(transformers.logging.ERROR)\n",
        "\n",
        "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We apply RoBERTuito for the older and younger patients' notes, respectively."
      ],
      "metadata": {
        "id": "j8IB96Ke964Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###Sentiment in texts for older:\n",
        "positive_count = 0\n",
        "neutral_count = 0\n",
        "negative_count = 0\n",
        "\n",
        "for index, row in older.iterrows():\n",
        "    text = row['text_y']\n",
        "    result_older = analyzer.predict(text)\n",
        "\n",
        "    if result_older.output == 'POS':\n",
        "        positive_count += 1\n",
        "    elif result_older.output == 'NEU':\n",
        "        neutral_count += 1\n",
        "    elif result_older.output == 'NEG':\n",
        "        negative_count += 1\n",
        "\n",
        "print(\"Positive:\", positive_count)\n",
        "print(\"Neutral:\", neutral_count)\n",
        "print(\"Negative:\", negative_count)"
      ],
      "metadata": {
        "id": "MWph2RDDV4jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Sentiment in texts for younger:\n",
        "positive_count = 0\n",
        "neutral_count = 0\n",
        "negative_count = 0\n",
        "\n",
        "for index, row in younger.iterrows():\n",
        "    text = row['text_y']\n",
        "    result_younger = analyzer.predict(text)\n",
        "\n",
        "    if result_younger.output == 'POS':\n",
        "        positive_count += 1\n",
        "    elif result_younger.output == 'NEU':\n",
        "        neutral_count += 1\n",
        "    elif result_younger.output == 'NEG':\n",
        "        negative_count += 1\n",
        "\n",
        "print(\"Positive:\", positive_count)\n",
        "print(\"Neutral:\", neutral_count)\n",
        "print(\"Negative:\", negative_count)"
      ],
      "metadata": {
        "id": "hvnzWuXoXHDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. AFINN with Spanish Lexicon**"
      ],
      "metadata": {
        "id": "eJbc4UMo-yWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We install and import the necessary libraries."
      ],
      "metadata": {
        "id": "7Y_2mgx5-UcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob\n",
        "from textblob import TextBlob, Word\n",
        "\n",
        "!pip install nltk\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import string\n",
        "import os\n",
        "stop_words = set(stopwords.words('spanish'))"
      ],
      "metadata": {
        "id": "YQenbJHzXtqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, we initialise an empty dictionary to store FINN lexicon. Afterwards, we load the AFINN lexicon from a CSV file into a DataFrame. Only the first two columns are used. the, we iterate through each row of the DataFrame, extracting the word and its sentiment score."
      ],
      "metadata": {
        "id": "mPnlMYg3-5_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "afinn = {}\n",
        "\n",
        "lexicon_df = pd.read_csv('/content/lexico_afinn.csv', header=0)\n",
        "lexicon_df = lexicon_df.iloc[:, :2]\n",
        "\n",
        "for index, row in lexicon_df.iterrows():\n",
        "    word = row['palabra']\n",
        "    sentiment_score_str = row['puntuacion']\n",
        "\n",
        "    try:\n",
        "        sentiment_score = float(sentiment_score_str)\n",
        "        afinn[word] = sentiment_score\n",
        "    except ValueError:\n",
        "        print(f\"Invalid sentiment score '{sentiment_score_str}' for word '{word}'. Skipping row.\")"
      ],
      "metadata": {
        "id": "A5FkDDjPbHYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a function that calculates the sentiment of the text. First, convert the text to lowercase and split it into words. Then, calculate the sentiment score by summing the scores of individual words using the AFINN dictionary."
      ],
      "metadata": {
        "id": "Eqh2la33--5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sentiment(text):\n",
        "    words = text.lower().split()\n",
        "    sentiment_score = sum(afinn.get(word, 0) for word in words)\n",
        "    if sentiment_score > 0:\n",
        "        return 'Positive'\n",
        "    elif sentiment_score < 0:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'"
      ],
      "metadata": {
        "id": "Z17Hwv5Iao04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also upload stopwords for Catalan and add them to those for Spanish."
      ],
      "metadata": {
        "id": "MGLzAblJ_AM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "catalan_stopwords = catalan_stopwords = [\n",
        "    'de', 'es', 'i', 'a', 'o', 'un', 'una', 'unes', 'uns', 'un', 'tot',\n",
        "    'també', 'altre', 'algun', 'alguna', 'alguns', 'algunes', 'ser', 'és',\n",
        "    'soc', 'ets', 'som', 'estic', 'està', 'estem', 'esteu', 'estan', 'com',\n",
        "    'en', 'per', 'perquè', 'per que', 'estat', 'estava', 'ans', 'abans',\n",
        "    'éssent', 'ambdós', 'però', 'per', 'poder', 'potser', 'puc', 'podem',\n",
        "    'podeu', 'poden', 'vaig', 'va', 'van', 'fer', 'faig', 'fa', 'fem',\n",
        "    'feu', 'fan', 'cada', 'fi', 'inclòs', 'primer', 'des de', 'conseguir',\n",
        "    'consegueixo', 'consigueix', 'consigueixes', 'conseguim', 'consigueixen',\n",
        "    'anar', 'haver', 'tenir', 'tinc', 'te', 'tenim', 'teniu', 'tene', 'el',\n",
        "    'la', 'les', 'els', 'seu', 'aquí', 'meu', 'teu', 'ells', 'elles', 'ens',\n",
        "    'nosaltres', 'vosaltres', 'si', 'dins', 'sols', 'solament', 'saber',\n",
        "    'saps', 'sap', 'sabem', 'sabeu', 'saben', 'últim', 'llarg', 'bastant',\n",
        "    'fas', 'molts', 'aquells', 'aquelles', 'seus', 'llavors', 'sota', 'dalt',\n",
        "    'ús', 'molt', 'era', 'eres', 'erem', 'eren', 'mode', 'bé', 'quant',\n",
        "    'quan', 'on', 'mentre', 'qui', 'amb', 'entre', 'sense', 'jo', 'aquell'\n",
        "]\n",
        "\n",
        "stop_words.update(catalan_stopwords)"
      ],
      "metadata": {
        "id": "wWbbPHM309wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a function to preprocess text data. The text is converted to lower case, punctuation removed, tokenised using TextBlob, lemmatised, and stopwords removed."
      ],
      "metadata": {
        "id": "WSr_DoVe_GMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = TextBlob(text).words\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    lemmatized_tokens = [Word(word).lemmatize() for word in tokens]\n",
        "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "older['preprocessed_text'] = older['text_y'].apply(preprocess_text)\n",
        "younger['preprocessed_text'] = younger['text_y'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "spTfpwf2bRi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We apply the calculate_sentiment function to the 'preprocessed_text' column of the two dataframes and store the sentiment labels in a new column 'sentiment'."
      ],
      "metadata": {
        "id": "VcVKF3Hm_sJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "older['sentiment'] = older['preprocessed_text'].apply(lambda text: calculate_sentiment(text))\n",
        "younger['sentiment'] = younger['preprocessed_text'].apply(lambda text: calculate_sentiment(text))"
      ],
      "metadata": {
        "id": "1A646G9QSUL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We count the number of occurrences of each sentiment label ('Positive', 'Negative', 'Neutral') for older patients' notes.\n"
      ],
      "metadata": {
        "id": "6anchmxs_1gY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_counts_older = older['sentiment'].value_counts()\n",
        "sentiment_counts_older"
      ],
      "metadata": {
        "id": "eRc-6JsLSecq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We count the number of occurrences of each sentiment label ('Positive', 'Negative', 'Neutral') for younger patients' notes."
      ],
      "metadata": {
        "id": "ksColVf__7WL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_counts_younger = younger['sentiment'].value_counts()\n",
        "sentiment_counts_younger"
      ],
      "metadata": {
        "id": "ERbRKim1So41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic modelling and document embedding"
      ],
      "metadata": {
        "id": "XNe1dQ5PV_4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LDA**"
      ],
      "metadata": {
        "id": "3Txq3fyw-4e6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We install and import the necessary libraries."
      ],
      "metadata": {
        "id": "gjAOPRBoBk9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models.ldamodel import LdaModel"
      ],
      "metadata": {
        "id": "2vMTyk_g0VKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use LDA for topic modelling, following the following steps:"
      ],
      "metadata": {
        "id": "XZwe1LvjLVf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Firstly we tokenize the text:\n",
        "def tokenize_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "#We apply the function to the preprocessed text:\n",
        "older['tokenized_text'] = older['preprocessed_text'].apply(tokenize_text)\n",
        "younger['tokenized_text'] = younger['preprocessed_text'].apply(tokenize_text)\n",
        "\n",
        "#We create dictionary representation of the documents\n",
        "dictionary = Dictionary(older['tokenized_text'])\n",
        "dictionary_non = Dictionary(younger['tokenized_text'])\n",
        "\n",
        "dictionary.filter_extremes(no_below=4, no_above=0.4)\n",
        "dictionary_non.filter_extremes(no_below=4, no_above=0.4)\n",
        "\n",
        "# Convert the dictionary to a bag of words corpus\n",
        "corpus_older = [dictionary.doc2bow(doc) for doc in older['tokenized_text']]\n",
        "corpus_younger = [dictionary_non.doc2bow(doc) for doc in younger['tokenized_text']]"
      ],
      "metadata": {
        "id": "L9EHpEAZTyRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we apply the LDA model, and represent 2 topics for older and younger patients' notes, respectively:"
      ],
      "metadata": {
        "id": "psqCygEMLQCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_topics = 2\n",
        "\n",
        "#LDA in older patients\n",
        "lda_older = LdaModel(corpus_older, num_topics=num_topics, id2word=dictionary, passes=10)\n",
        "\n",
        "#LDA in younger patients\n",
        "lda_younger = LdaModel(corpus_younger, num_topics=num_topics, id2word=dictionary_non, passes=10)\n",
        "\n",
        "#Topics for the older patients\n",
        "print(\"Older Topics:\")\n",
        "for i, topic in lda_older.print_topics(num_words=10):\n",
        "    print(f\"Topic {i}: {topic}\")\n",
        "\n",
        "#Topics for the younger patients\n",
        "print(\"Younger Topics:\")\n",
        "for i, topic in lda_younger.print_topics(num_words=10):\n",
        "    print(f\"Topic {i}: {topic}\")"
      ],
      "metadata": {
        "id": "sBFdrVNKVu1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**t-SNE**"
      ],
      "metadata": {
        "id": "NAgZAwIyFpkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We install and import the necessary libraries:"
      ],
      "metadata": {
        "id": "kunam2hlLj0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas scikit-learn\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "UQJEtIA6FrDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We preprocess the text in the age_df dataframe, and then we vectorise it."
      ],
      "metadata": {
        "id": "lbTDk6GuMGB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "age_df['preprocessed_text'] = age_df['text_y'].apply(preprocess_text)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(age_df['preprocessed_text'])"
      ],
      "metadata": {
        "id": "yRWCwCqRFvhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We apply T-SNE and plot it."
      ],
      "metadata": {
        "id": "eTRSWRepML0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "\n",
        "tsne_results = tsne.fit_transform(tfidf_matrix.toarray())\n",
        "\n",
        "age_df['tsne_1'] = tsne_results[:, 0]\n",
        "age_df['tsne_2'] = tsne_results[:, 1]\n",
        "\n",
        "#plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = sns.scatterplot(\n",
        "    x='tsne_1', y='tsne_2',\n",
        "    hue='older',\n",
        "    palette=sns.color_palette(\"rocket\", 2),\n",
        "    data=age_df,\n",
        "    legend=\"full\",\n",
        "    alpha=0.6\n",
        ")\n",
        "\n",
        "handles, labels = scatter.get_legend_handles_labels()\n",
        "labels = ['Younger', 'Older']\n",
        "scatter.legend(handles, labels, title=\"Age\")\n",
        "\n",
        "plt.title('t-SNE visualization of topics for each age group')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SvCBckK7F8iB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic regression\n",
        "\n",
        "We intend to predict if the patient category older or younger, from the text. Firstly, we import the necessary libraries."
      ],
      "metadata": {
        "id": "2Z4DiCX1vPnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "3GP2vvL8vbT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we follow the next steps to train the logistic regression model, make predictions, and get a confusion matrix and accuracy score."
      ],
      "metadata": {
        "id": "j0gYbP6eOFot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(age_df['text_y'], age_df['older'], test_size=0.2, random_state=42)\n",
        "\n",
        "#Preprocess text and convert to numerical features using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "#Train logistic regression model\n",
        "logreg_model = LogisticRegression()\n",
        "logreg_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "#Make predictions on the test set\n",
        "predictions = logreg_model.predict(X_test_tfidf)\n",
        "\n",
        "#Evaluate predictions\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "#confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "true_positives = conf_matrix[1, 1]\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"True Positives:\", true_positives)"
      ],
      "metadata": {
        "id": "OhItW8izw7G3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we build a classification report, and we get the accuracy for the groups older patients and younger patients."
      ],
      "metadata": {
        "id": "3Zhj7IPmOe6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_report = classification_report(y_test, predictions, target_names=['Younger', 'Older'])\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n",
        "\n",
        "# Extract values from the confusion matrix\n",
        "TN = conf_matrix[0, 0]\n",
        "FP = conf_matrix[0, 1]\n",
        "FN = conf_matrix[1, 0]\n",
        "TP = conf_matrix[1, 1]\n",
        "\n",
        "# Calculate accuracy for each group\n",
        "accuracy_younger = TN / (TN + FP)\n",
        "accuracy_older = TP / (TP + FN)\n",
        "\n",
        "print(f'Accuracy for texts from younger: {accuracy_younger}')\n",
        "print(f'Accuracy for texts from older: {accuracy_older}')"
      ],
      "metadata": {
        "id": "r2F0z9OFzzus"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}